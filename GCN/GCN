{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1JjQfO7w-mpKINjNrh-Aavb-ZxT0tNqjH","authorship_tag":"ABX9TyP+cjvQ/DcO2mN7/ZQg0DEc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["##Load citation dataset: Citeseer, Cora, Pubmed (node = document, edge = citation links)\n","##Load Nell dataset (knowledge graph)"],"metadata":{"id":"DLebNVdTQh6Z"}},{"cell_type":"code","source":["import numpy as np\n","import scipy.sparse as sp\n","import torch\n","\n","\n","def encode_onehot(labels):\n","    classes = set(labels)\n","    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n","                    enumerate(classes)}\n","    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n","                             dtype=np.int32)\n","    return labels_onehot\n","\n","\n","def load_data(path=\"/content/drive/MyDrive/cora/\", dataset=\"cora\"):\n","    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n","    print('Loading {} dataset...'.format(dataset))\n","\n","    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n","                                        dtype=np.dtype(str))\n","    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n","    labels = encode_onehot(idx_features_labels[:, -1])\n","\n","    # build graph\n","    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n","    idx_map = {j: i for i, j in enumerate(idx)}\n","    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n","                                    dtype=np.int32)\n","    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n","                     dtype=np.int32).reshape(edges_unordered.shape)\n","    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n","                        shape=(labels.shape[0], labels.shape[0]),\n","                        dtype=np.float32)\n","\n","    # build symmetric adjacency matrix\n","    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n","\n","    features = normalize(features)\n","    adj = normalize(adj + sp.eye(adj.shape[0]))\n","\n","    idx_train = range(140)\n","    idx_val = range(200, 500)\n","    idx_test = range(500, 1500)\n","\n","    features = torch.FloatTensor(np.array(features.todense()))\n","    labels = torch.LongTensor(np.where(labels)[1])\n","    adj = sparse_mx_to_torch_sparse_tensor(adj)\n","\n","    idx_train = torch.LongTensor(idx_train)\n","    idx_val = torch.LongTensor(idx_val)\n","    idx_test = torch.LongTensor(idx_test)\n","\n","    return adj, features, labels, idx_train, idx_val, idx_test\n","\n","\n","def normalize(mx):\n","    \"\"\"Row-normalize sparse matrix\"\"\"\n","    rowsum = np.array(mx.sum(1))\n","    r_inv = np.power(rowsum, -1).flatten()\n","    r_inv[np.isinf(r_inv)] = 0.\n","    r_mat_inv = sp.diags(r_inv)\n","    mx = r_mat_inv.dot(mx)\n","    return mx\n","\n","\n","def accuracy(output, labels):\n","    preds = output.max(1)[1].type_as(labels)\n","    correct = preds.eq(labels).double()\n","    correct = correct.sum()\n","    return correct / len(labels)\n","\n","\n","def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n","    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n","    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n","    indices = torch.from_numpy(\n","        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n","    values = torch.from_numpy(sparse_mx.data)\n","    shape = torch.Size(sparse_mx.shape)\n","    return torch.sparse.FloatTensor(indices, values, shape)\n"],"metadata":{"id":"GJWggDkk_FLU","executionInfo":{"status":"ok","timestamp":1678644632388,"user_tz":-540,"elapsed":3,"user":{"displayName":"강홍석","userId":"01063535304266029944"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["from torch.nn.parameter import Parameter\n","from torch.nn.modules.module import Module\n","\n","class GraphConvolution(Module):\n","    \"\"\"\n","    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n","    \"\"\"\n","\n","    def __init__(self, in_features, out_features, bias=True):\n","        super(GraphConvolution, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n","        if bias:\n","            self.bias = Parameter(torch.FloatTensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        stdv = 1. / math.sqrt(self.weight.size(1))\n","        self.weight.data.uniform_(-stdv, stdv)\n","        if self.bias is not None:\n","            self.bias.data.uniform_(-stdv, stdv)\n","\n","    def forward(self, input, adj):\n","        support = torch.mm(input, self.weight)\n","        output = torch.spmm(adj, support)\n","        if self.bias is not None:\n","            return output + self.bias\n","        else:\n","            return output\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + ' (' \\\n","               + str(self.in_features) + ' -> ' \\\n","               + str(self.out_features) + ')'"],"metadata":{"id":"WUOxGazJ-Nev","executionInfo":{"status":"ok","timestamp":1678644594544,"user_tz":-540,"elapsed":2,"user":{"displayName":"강홍석","userId":"01063535304266029944"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class GCN(nn.Module):\n","    def __init__(self, nfeat, nhid, nclass, dropout):\n","        super(GCN, self).__init__()\n","\n","        self.gc1 = GraphConvolution(nfeat, nhid)\n","        self.gc2 = GraphConvolution(nhid, nclass)\n","        self.dropout = dropout\n","\n","    def forward(self, x, adj):\n","        x = F.relu(self.gc1(x, adj))\n","        x = F.dropout(x, self.dropout, training=self.training)\n","        x = self.gc2(x, adj)\n","        return F.log_softmax(x, dim=1)"],"metadata":{"id":"1LhMz_BwBZMJ","executionInfo":{"status":"ok","timestamp":1678644720774,"user_tz":-540,"elapsed":327,"user":{"displayName":"강홍석","userId":"01063535304266029944"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","adj, features, labels, idx_train, idx_val, idx_test = load_data()\n","\n","# Model and optimizer\n","model = GCN(nfeat=features.shape[1],\n","            nhid=16,\n","            nclass=labels.max().item() + 1,\n","            dropout=0.5)\n","\n","optimizer = optim.Adam(model.parameters(),\n","                       lr=0.01)\n","\n","def train(epoch):\n","    t = time.time()\n","    model.train()\n","    optimizer.zero_grad()\n","    output = model(features, adj)\n","    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n","    acc_train = accuracy(output[idx_train], labels[idx_train])\n","    loss_train.backward()\n","    optimizer.step()\n","\n","    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n","    acc_val = accuracy(output[idx_val], labels[idx_val])\n","    print('Epoch: {:04d}'.format(epoch+1),\n","          'loss_train: {:.4f}'.format(loss_train.item()),\n","          'acc_train: {:.4f}'.format(acc_train.item()),\n","          'loss_val: {:.4f}'.format(loss_val.item()),\n","          'acc_val: {:.4f}'.format(acc_val.item()),\n","          'time: {:.4f}s'.format(time.time() - t))\n","\n","\n","def test():\n","    model.eval()\n","    output = model(features, adj)\n","    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n","    acc_test = accuracy(output[idx_test], labels[idx_test])\n","    print(\"Test set results:\",\n","          \"loss= {:.4f}\".format(loss_test.item()),\n","          \"accuracy= {:.4f}\".format(acc_test.item()))\n","\n","\n","# Train model\n","t_total = time.time()\n","for epoch in range(200):\n","    train(epoch)\n","print(\"Optimization Finished!\")\n","print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n","\n","# Testing\n","test()"],"metadata":{"id":"HuRdQKsaWzzV","executionInfo":{"status":"ok","timestamp":1678644734151,"user_tz":-540,"elapsed":11026,"user":{"displayName":"강홍석","userId":"01063535304266029944"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f7f59d80-052e-4cd3-ed3e-a75742bc685d"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cora dataset...\n","Epoch: 0001 loss_train: 2.0060 acc_train: 0.1357 loss_val: 2.0242 acc_val: 0.1000 time: 0.1327s\n","Epoch: 0002 loss_train: 1.9903 acc_train: 0.1214 loss_val: 2.0074 acc_val: 0.1033 time: 0.0130s\n","Epoch: 0003 loss_train: 1.9749 acc_train: 0.1286 loss_val: 1.9898 acc_val: 0.1200 time: 0.0129s\n","Epoch: 0004 loss_train: 1.9566 acc_train: 0.1214 loss_val: 1.9719 acc_val: 0.1067 time: 0.0131s\n","Epoch: 0005 loss_train: 1.9407 acc_train: 0.1857 loss_val: 1.9582 acc_val: 0.1467 time: 0.0128s\n","Epoch: 0006 loss_train: 1.9349 acc_train: 0.1357 loss_val: 1.9440 acc_val: 0.1200 time: 0.0134s\n","Epoch: 0007 loss_train: 1.9220 acc_train: 0.1286 loss_val: 1.9345 acc_val: 0.1033 time: 0.0171s\n","Epoch: 0008 loss_train: 1.9021 acc_train: 0.1500 loss_val: 1.9194 acc_val: 0.1533 time: 0.0132s\n","Epoch: 0009 loss_train: 1.8861 acc_train: 0.2357 loss_val: 1.8941 acc_val: 0.2333 time: 0.0132s\n","Epoch: 0010 loss_train: 1.8833 acc_train: 0.2857 loss_val: 1.8860 acc_val: 0.2700 time: 0.0129s\n","Epoch: 0011 loss_train: 1.8669 acc_train: 0.2786 loss_val: 1.8747 acc_val: 0.3100 time: 0.0130s\n","Epoch: 0012 loss_train: 1.8511 acc_train: 0.2714 loss_val: 1.8619 acc_val: 0.3067 time: 0.0129s\n","Epoch: 0013 loss_train: 1.8389 acc_train: 0.3214 loss_val: 1.8566 acc_val: 0.3400 time: 0.0130s\n","Epoch: 0014 loss_train: 1.8302 acc_train: 0.3214 loss_val: 1.8369 acc_val: 0.3433 time: 0.0159s\n","Epoch: 0015 loss_train: 1.8107 acc_train: 0.3357 loss_val: 1.8248 acc_val: 0.3333 time: 0.0135s\n","Epoch: 0016 loss_train: 1.8019 acc_train: 0.3429 loss_val: 1.8103 acc_val: 0.3533 time: 0.0140s\n","Epoch: 0017 loss_train: 1.8078 acc_train: 0.3000 loss_val: 1.8174 acc_val: 0.3367 time: 0.0131s\n","Epoch: 0018 loss_train: 1.7660 acc_train: 0.3357 loss_val: 1.7895 acc_val: 0.3500 time: 0.0135s\n","Epoch: 0019 loss_train: 1.7831 acc_train: 0.3214 loss_val: 1.7981 acc_val: 0.3533 time: 0.0134s\n","Epoch: 0020 loss_train: 1.7475 acc_train: 0.3214 loss_val: 1.7740 acc_val: 0.3533 time: 0.0153s\n","Epoch: 0021 loss_train: 1.7409 acc_train: 0.3143 loss_val: 1.7487 acc_val: 0.3567 time: 0.0133s\n","Epoch: 0022 loss_train: 1.7352 acc_train: 0.3143 loss_val: 1.7439 acc_val: 0.3533 time: 0.0143s\n","Epoch: 0023 loss_train: 1.7259 acc_train: 0.3071 loss_val: 1.7393 acc_val: 0.3500 time: 0.0143s\n","Epoch: 0024 loss_train: 1.6936 acc_train: 0.3000 loss_val: 1.7158 acc_val: 0.3600 time: 0.0133s\n","Epoch: 0025 loss_train: 1.7201 acc_train: 0.3143 loss_val: 1.7142 acc_val: 0.3567 time: 0.0136s\n","Epoch: 0026 loss_train: 1.6923 acc_train: 0.3357 loss_val: 1.7105 acc_val: 0.3700 time: 0.0130s\n","Epoch: 0027 loss_train: 1.6825 acc_train: 0.3429 loss_val: 1.7112 acc_val: 0.3533 time: 0.0170s\n","Epoch: 0028 loss_train: 1.6892 acc_train: 0.3143 loss_val: 1.7087 acc_val: 0.3533 time: 0.0132s\n","Epoch: 0029 loss_train: 1.6175 acc_train: 0.3500 loss_val: 1.6720 acc_val: 0.3633 time: 0.0134s\n","Epoch: 0030 loss_train: 1.6393 acc_train: 0.3786 loss_val: 1.6802 acc_val: 0.3667 time: 0.0138s\n","Epoch: 0031 loss_train: 1.6470 acc_train: 0.3286 loss_val: 1.6963 acc_val: 0.3600 time: 0.0125s\n","Epoch: 0032 loss_train: 1.6399 acc_train: 0.3500 loss_val: 1.6802 acc_val: 0.3633 time: 0.0130s\n","Epoch: 0033 loss_train: 1.5775 acc_train: 0.3571 loss_val: 1.6480 acc_val: 0.3833 time: 0.0157s\n","Epoch: 0034 loss_train: 1.5475 acc_train: 0.3857 loss_val: 1.6246 acc_val: 0.3600 time: 0.0127s\n","Epoch: 0035 loss_train: 1.5531 acc_train: 0.3857 loss_val: 1.6049 acc_val: 0.4233 time: 0.0132s\n","Epoch: 0036 loss_train: 1.5354 acc_train: 0.4143 loss_val: 1.5851 acc_val: 0.3967 time: 0.0124s\n","Epoch: 0037 loss_train: 1.5399 acc_train: 0.3714 loss_val: 1.6158 acc_val: 0.4100 time: 0.0127s\n","Epoch: 0038 loss_train: 1.5128 acc_train: 0.4429 loss_val: 1.5952 acc_val: 0.4133 time: 0.0131s\n","Epoch: 0039 loss_train: 1.4900 acc_train: 0.4714 loss_val: 1.5929 acc_val: 0.4267 time: 0.0142s\n","Epoch: 0040 loss_train: 1.5079 acc_train: 0.4857 loss_val: 1.5950 acc_val: 0.4500 time: 0.0263s\n","Epoch: 0041 loss_train: 1.4698 acc_train: 0.4571 loss_val: 1.5402 acc_val: 0.4533 time: 0.0169s\n","Epoch: 0042 loss_train: 1.4626 acc_train: 0.4929 loss_val: 1.5718 acc_val: 0.4433 time: 0.0172s\n","Epoch: 0043 loss_train: 1.4466 acc_train: 0.5071 loss_val: 1.5301 acc_val: 0.4400 time: 0.0183s\n","Epoch: 0044 loss_train: 1.3985 acc_train: 0.5357 loss_val: 1.5603 acc_val: 0.4900 time: 0.0165s\n","Epoch: 0045 loss_train: 1.4231 acc_train: 0.5571 loss_val: 1.5430 acc_val: 0.4967 time: 0.0167s\n","Epoch: 0046 loss_train: 1.3821 acc_train: 0.5000 loss_val: 1.4952 acc_val: 0.4800 time: 0.0165s\n","Epoch: 0047 loss_train: 1.3599 acc_train: 0.5357 loss_val: 1.5026 acc_val: 0.5167 time: 0.0168s\n","Epoch: 0048 loss_train: 1.3192 acc_train: 0.5929 loss_val: 1.4954 acc_val: 0.5233 time: 0.0171s\n","Epoch: 0049 loss_train: 1.3107 acc_train: 0.5786 loss_val: 1.4517 acc_val: 0.5067 time: 0.0177s\n","Epoch: 0050 loss_train: 1.3016 acc_train: 0.6071 loss_val: 1.4147 acc_val: 0.5400 time: 0.0173s\n","Epoch: 0051 loss_train: 1.3133 acc_train: 0.6143 loss_val: 1.4387 acc_val: 0.5300 time: 0.0225s\n","Epoch: 0052 loss_train: 1.2925 acc_train: 0.5857 loss_val: 1.4701 acc_val: 0.4733 time: 0.0161s\n","Epoch: 0053 loss_train: 1.2284 acc_train: 0.6571 loss_val: 1.4128 acc_val: 0.5667 time: 0.0195s\n","Epoch: 0054 loss_train: 1.2119 acc_train: 0.6500 loss_val: 1.3948 acc_val: 0.5600 time: 0.0173s\n","Epoch: 0055 loss_train: 1.1949 acc_train: 0.6857 loss_val: 1.3930 acc_val: 0.5667 time: 0.0190s\n","Epoch: 0056 loss_train: 1.1918 acc_train: 0.6571 loss_val: 1.3994 acc_val: 0.5600 time: 0.0177s\n","Epoch: 0057 loss_train: 1.1727 acc_train: 0.7214 loss_val: 1.3550 acc_val: 0.6000 time: 0.0173s\n","Epoch: 0058 loss_train: 1.1520 acc_train: 0.6714 loss_val: 1.3488 acc_val: 0.5900 time: 0.0164s\n","Epoch: 0059 loss_train: 1.0674 acc_train: 0.7429 loss_val: 1.2968 acc_val: 0.6133 time: 0.0173s\n","Epoch: 0060 loss_train: 1.0853 acc_train: 0.7000 loss_val: 1.3328 acc_val: 0.5700 time: 0.0171s\n","Epoch: 0061 loss_train: 1.0459 acc_train: 0.7143 loss_val: 1.2938 acc_val: 0.5933 time: 0.0223s\n","Epoch: 0062 loss_train: 1.0437 acc_train: 0.7500 loss_val: 1.2758 acc_val: 0.6033 time: 0.0170s\n","Epoch: 0063 loss_train: 1.0260 acc_train: 0.7286 loss_val: 1.2951 acc_val: 0.5833 time: 0.0163s\n","Epoch: 0064 loss_train: 1.0501 acc_train: 0.7357 loss_val: 1.2613 acc_val: 0.6233 time: 0.0163s\n","Epoch: 0065 loss_train: 1.0033 acc_train: 0.7357 loss_val: 1.2616 acc_val: 0.6300 time: 0.0162s\n","Epoch: 0066 loss_train: 0.9682 acc_train: 0.7643 loss_val: 1.2291 acc_val: 0.6000 time: 0.0170s\n","Epoch: 0067 loss_train: 0.9325 acc_train: 0.7857 loss_val: 1.2099 acc_val: 0.6400 time: 0.0197s\n","Epoch: 0068 loss_train: 0.9509 acc_train: 0.7786 loss_val: 1.2027 acc_val: 0.6400 time: 0.0165s\n","Epoch: 0069 loss_train: 0.9018 acc_train: 0.7714 loss_val: 1.1911 acc_val: 0.6500 time: 0.0168s\n","Epoch: 0070 loss_train: 0.9656 acc_train: 0.7714 loss_val: 1.1644 acc_val: 0.6600 time: 0.0170s\n","Epoch: 0071 loss_train: 0.8960 acc_train: 0.7786 loss_val: 1.1913 acc_val: 0.6433 time: 0.0177s\n","Epoch: 0072 loss_train: 0.8908 acc_train: 0.8143 loss_val: 1.1854 acc_val: 0.6567 time: 0.0214s\n","Epoch: 0073 loss_train: 0.8795 acc_train: 0.8000 loss_val: 1.1543 acc_val: 0.6733 time: 0.0159s\n","Epoch: 0074 loss_train: 0.8278 acc_train: 0.8214 loss_val: 1.1421 acc_val: 0.6733 time: 0.0181s\n","Epoch: 0075 loss_train: 0.7781 acc_train: 0.8143 loss_val: 1.0973 acc_val: 0.6567 time: 0.0177s\n","Epoch: 0076 loss_train: 0.8195 acc_train: 0.8143 loss_val: 1.1194 acc_val: 0.6900 time: 0.0175s\n","Epoch: 0077 loss_train: 0.7794 acc_train: 0.8143 loss_val: 1.0923 acc_val: 0.7000 time: 0.0173s\n","Epoch: 0078 loss_train: 0.7999 acc_train: 0.8071 loss_val: 1.0473 acc_val: 0.7000 time: 0.0185s\n","Epoch: 0079 loss_train: 0.7858 acc_train: 0.8429 loss_val: 1.1042 acc_val: 0.6733 time: 0.0177s\n","Epoch: 0080 loss_train: 0.7976 acc_train: 0.7857 loss_val: 1.0590 acc_val: 0.6967 time: 0.0174s\n","Epoch: 0081 loss_train: 0.7458 acc_train: 0.8429 loss_val: 1.0978 acc_val: 0.6733 time: 0.0173s\n","Epoch: 0082 loss_train: 0.7173 acc_train: 0.8071 loss_val: 1.0216 acc_val: 0.6867 time: 0.0186s\n","Epoch: 0083 loss_train: 0.7161 acc_train: 0.8643 loss_val: 1.0767 acc_val: 0.7200 time: 0.0199s\n","Epoch: 0084 loss_train: 0.7704 acc_train: 0.8000 loss_val: 1.0406 acc_val: 0.6833 time: 0.0166s\n","Epoch: 0085 loss_train: 0.6909 acc_train: 0.8143 loss_val: 1.0236 acc_val: 0.7100 time: 0.0178s\n","Epoch: 0086 loss_train: 0.6676 acc_train: 0.8500 loss_val: 1.0474 acc_val: 0.6767 time: 0.0171s\n","Epoch: 0087 loss_train: 0.6441 acc_train: 0.8714 loss_val: 1.0084 acc_val: 0.6967 time: 0.0177s\n","Epoch: 0088 loss_train: 0.6218 acc_train: 0.8929 loss_val: 1.0252 acc_val: 0.7033 time: 0.0184s\n","Epoch: 0089 loss_train: 0.6660 acc_train: 0.8571 loss_val: 1.0206 acc_val: 0.6667 time: 0.0247s\n","Epoch: 0090 loss_train: 0.6313 acc_train: 0.8643 loss_val: 0.9575 acc_val: 0.7367 time: 0.0132s\n","Epoch: 0091 loss_train: 0.6390 acc_train: 0.8071 loss_val: 1.0031 acc_val: 0.7033 time: 0.0147s\n","Epoch: 0092 loss_train: 0.6012 acc_train: 0.8643 loss_val: 0.9852 acc_val: 0.6767 time: 0.0158s\n","Epoch: 0093 loss_train: 0.6382 acc_train: 0.8429 loss_val: 0.9575 acc_val: 0.7200 time: 0.0127s\n","Epoch: 0094 loss_train: 0.6059 acc_train: 0.8429 loss_val: 0.9537 acc_val: 0.7067 time: 0.0134s\n","Epoch: 0095 loss_train: 0.6153 acc_train: 0.8214 loss_val: 0.9226 acc_val: 0.6967 time: 0.0134s\n","Epoch: 0096 loss_train: 0.5830 acc_train: 0.8357 loss_val: 0.9233 acc_val: 0.7267 time: 0.0131s\n","Epoch: 0097 loss_train: 0.5922 acc_train: 0.8786 loss_val: 0.9381 acc_val: 0.7367 time: 0.0135s\n","Epoch: 0098 loss_train: 0.5900 acc_train: 0.8571 loss_val: 0.9636 acc_val: 0.7200 time: 0.0146s\n","Epoch: 0099 loss_train: 0.5351 acc_train: 0.8714 loss_val: 0.9262 acc_val: 0.7067 time: 0.0131s\n","Epoch: 0100 loss_train: 0.5222 acc_train: 0.8714 loss_val: 0.9085 acc_val: 0.7233 time: 0.0144s\n","Epoch: 0101 loss_train: 0.5695 acc_train: 0.8786 loss_val: 0.9729 acc_val: 0.7100 time: 0.0195s\n","Epoch: 0102 loss_train: 0.5732 acc_train: 0.8571 loss_val: 0.9428 acc_val: 0.6833 time: 0.0124s\n","Epoch: 0103 loss_train: 0.4638 acc_train: 0.9143 loss_val: 0.9077 acc_val: 0.7467 time: 0.0129s\n","Epoch: 0104 loss_train: 0.5095 acc_train: 0.8571 loss_val: 0.9509 acc_val: 0.7067 time: 0.0176s\n","Epoch: 0105 loss_train: 0.5449 acc_train: 0.8714 loss_val: 0.8785 acc_val: 0.7500 time: 0.0132s\n","Epoch: 0106 loss_train: 0.4715 acc_train: 0.8714 loss_val: 0.9241 acc_val: 0.7300 time: 0.0131s\n","Epoch: 0107 loss_train: 0.4806 acc_train: 0.9000 loss_val: 0.8677 acc_val: 0.7667 time: 0.0132s\n","Epoch: 0108 loss_train: 0.4718 acc_train: 0.9000 loss_val: 0.8705 acc_val: 0.7400 time: 0.0136s\n","Epoch: 0109 loss_train: 0.5187 acc_train: 0.8786 loss_val: 0.8775 acc_val: 0.7100 time: 0.0132s\n","Epoch: 0110 loss_train: 0.5201 acc_train: 0.8429 loss_val: 0.9153 acc_val: 0.7167 time: 0.0135s\n","Epoch: 0111 loss_train: 0.4879 acc_train: 0.9071 loss_val: 0.8605 acc_val: 0.7333 time: 0.0134s\n","Epoch: 0112 loss_train: 0.4345 acc_train: 0.9071 loss_val: 0.8965 acc_val: 0.6967 time: 0.0133s\n","Epoch: 0113 loss_train: 0.4362 acc_train: 0.9357 loss_val: 0.8935 acc_val: 0.7100 time: 0.0129s\n","Epoch: 0114 loss_train: 0.4236 acc_train: 0.8857 loss_val: 0.8516 acc_val: 0.7500 time: 0.0132s\n","Epoch: 0115 loss_train: 0.4089 acc_train: 0.9071 loss_val: 0.8508 acc_val: 0.7400 time: 0.0157s\n","Epoch: 0116 loss_train: 0.3849 acc_train: 0.9357 loss_val: 0.8749 acc_val: 0.7233 time: 0.0138s\n","Epoch: 0117 loss_train: 0.4142 acc_train: 0.8786 loss_val: 0.8937 acc_val: 0.7267 time: 0.0165s\n","Epoch: 0118 loss_train: 0.4405 acc_train: 0.8929 loss_val: 0.8989 acc_val: 0.7100 time: 0.0137s\n","Epoch: 0119 loss_train: 0.3944 acc_train: 0.9143 loss_val: 0.9012 acc_val: 0.7000 time: 0.0136s\n","Epoch: 0120 loss_train: 0.4164 acc_train: 0.9143 loss_val: 0.8394 acc_val: 0.7367 time: 0.0152s\n","Epoch: 0121 loss_train: 0.3837 acc_train: 0.9143 loss_val: 0.9256 acc_val: 0.7200 time: 0.0160s\n","Epoch: 0122 loss_train: 0.4398 acc_train: 0.8929 loss_val: 0.8780 acc_val: 0.7233 time: 0.0138s\n","Epoch: 0123 loss_train: 0.4061 acc_train: 0.9143 loss_val: 0.8602 acc_val: 0.7367 time: 0.0136s\n","Epoch: 0124 loss_train: 0.3919 acc_train: 0.8857 loss_val: 0.8265 acc_val: 0.7633 time: 0.0135s\n","Epoch: 0125 loss_train: 0.4085 acc_train: 0.9143 loss_val: 0.8453 acc_val: 0.7067 time: 0.0131s\n","Epoch: 0126 loss_train: 0.4135 acc_train: 0.9000 loss_val: 0.8993 acc_val: 0.7200 time: 0.0133s\n","Epoch: 0127 loss_train: 0.3867 acc_train: 0.9143 loss_val: 0.8411 acc_val: 0.7333 time: 0.0136s\n","Epoch: 0128 loss_train: 0.4393 acc_train: 0.8929 loss_val: 0.8057 acc_val: 0.7600 time: 0.0135s\n","Epoch: 0129 loss_train: 0.4100 acc_train: 0.8786 loss_val: 0.8451 acc_val: 0.7100 time: 0.0132s\n","Epoch: 0130 loss_train: 0.3849 acc_train: 0.9143 loss_val: 0.8299 acc_val: 0.7300 time: 0.0153s\n","Epoch: 0131 loss_train: 0.4157 acc_train: 0.9000 loss_val: 0.7930 acc_val: 0.7500 time: 0.0135s\n","Epoch: 0132 loss_train: 0.3117 acc_train: 0.9357 loss_val: 0.8123 acc_val: 0.7667 time: 0.0136s\n","Epoch: 0133 loss_train: 0.3393 acc_train: 0.9286 loss_val: 0.7870 acc_val: 0.7533 time: 0.0135s\n","Epoch: 0134 loss_train: 0.3263 acc_train: 0.9571 loss_val: 0.7799 acc_val: 0.7433 time: 0.0135s\n","Epoch: 0135 loss_train: 0.3256 acc_train: 0.9500 loss_val: 0.8485 acc_val: 0.7300 time: 0.0139s\n","Epoch: 0136 loss_train: 0.3228 acc_train: 0.9500 loss_val: 0.8755 acc_val: 0.7200 time: 0.0129s\n","Epoch: 0137 loss_train: 0.3285 acc_train: 0.9429 loss_val: 0.8133 acc_val: 0.7667 time: 0.0130s\n","Epoch: 0138 loss_train: 0.3486 acc_train: 0.9429 loss_val: 0.8718 acc_val: 0.7233 time: 0.0136s\n","Epoch: 0139 loss_train: 0.2993 acc_train: 0.9571 loss_val: 0.8295 acc_val: 0.7300 time: 0.0131s\n","Epoch: 0140 loss_train: 0.3490 acc_train: 0.9214 loss_val: 0.8892 acc_val: 0.7433 time: 0.0130s\n","Epoch: 0141 loss_train: 0.3541 acc_train: 0.8857 loss_val: 0.8311 acc_val: 0.7600 time: 0.0132s\n","Epoch: 0142 loss_train: 0.3571 acc_train: 0.9286 loss_val: 0.8129 acc_val: 0.7533 time: 0.0154s\n","Epoch: 0143 loss_train: 0.3359 acc_train: 0.9429 loss_val: 0.7729 acc_val: 0.7667 time: 0.0168s\n","Epoch: 0144 loss_train: 0.3003 acc_train: 0.9500 loss_val: 0.8404 acc_val: 0.7533 time: 0.0133s\n","Epoch: 0145 loss_train: 0.3602 acc_train: 0.9357 loss_val: 0.8779 acc_val: 0.7400 time: 0.0131s\n","Epoch: 0146 loss_train: 0.3439 acc_train: 0.9286 loss_val: 0.8399 acc_val: 0.7333 time: 0.0125s\n","Epoch: 0147 loss_train: 0.3246 acc_train: 0.9429 loss_val: 0.9347 acc_val: 0.6900 time: 0.0127s\n","Epoch: 0148 loss_train: 0.3266 acc_train: 0.9357 loss_val: 0.8392 acc_val: 0.7400 time: 0.0130s\n","Epoch: 0149 loss_train: 0.2788 acc_train: 0.9357 loss_val: 0.7803 acc_val: 0.7400 time: 0.0135s\n","Epoch: 0150 loss_train: 0.3230 acc_train: 0.9286 loss_val: 0.8389 acc_val: 0.7700 time: 0.0133s\n","Epoch: 0151 loss_train: 0.3019 acc_train: 0.9500 loss_val: 0.8404 acc_val: 0.7200 time: 0.0206s\n","Epoch: 0152 loss_train: 0.3170 acc_train: 0.9500 loss_val: 0.7723 acc_val: 0.7700 time: 0.0131s\n","Epoch: 0153 loss_train: 0.3211 acc_train: 0.9214 loss_val: 0.8587 acc_val: 0.7467 time: 0.0142s\n","Epoch: 0154 loss_train: 0.2956 acc_train: 0.9571 loss_val: 0.7937 acc_val: 0.7567 time: 0.0131s\n","Epoch: 0155 loss_train: 0.2988 acc_train: 0.9500 loss_val: 0.7699 acc_val: 0.7500 time: 0.0131s\n","Epoch: 0156 loss_train: 0.2914 acc_train: 0.9571 loss_val: 0.7788 acc_val: 0.7467 time: 0.0171s\n","Epoch: 0157 loss_train: 0.3027 acc_train: 0.9571 loss_val: 0.8072 acc_val: 0.7667 time: 0.0129s\n","Epoch: 0158 loss_train: 0.2613 acc_train: 0.9786 loss_val: 0.8311 acc_val: 0.7500 time: 0.0126s\n","Epoch: 0159 loss_train: 0.2708 acc_train: 0.9429 loss_val: 0.8432 acc_val: 0.7600 time: 0.0129s\n","Epoch: 0160 loss_train: 0.2889 acc_train: 0.9286 loss_val: 0.8999 acc_val: 0.7233 time: 0.0135s\n","Epoch: 0161 loss_train: 0.2206 acc_train: 0.9786 loss_val: 0.8306 acc_val: 0.7633 time: 0.0131s\n","Epoch: 0162 loss_train: 0.2723 acc_train: 0.9429 loss_val: 0.8424 acc_val: 0.7467 time: 0.0152s\n","Epoch: 0163 loss_train: 0.2737 acc_train: 0.9500 loss_val: 0.7917 acc_val: 0.7733 time: 0.0117s\n","Epoch: 0164 loss_train: 0.2488 acc_train: 0.9429 loss_val: 0.8088 acc_val: 0.7500 time: 0.0122s\n","Epoch: 0165 loss_train: 0.2428 acc_train: 0.9643 loss_val: 0.8500 acc_val: 0.7467 time: 0.0130s\n","Epoch: 0166 loss_train: 0.2630 acc_train: 0.9429 loss_val: 0.8132 acc_val: 0.7433 time: 0.0144s\n","Epoch: 0167 loss_train: 0.2346 acc_train: 0.9500 loss_val: 0.8179 acc_val: 0.7367 time: 0.0140s\n","Epoch: 0168 loss_train: 0.2892 acc_train: 0.9357 loss_val: 0.8356 acc_val: 0.7500 time: 0.0140s\n","Epoch: 0169 loss_train: 0.2845 acc_train: 0.9571 loss_val: 0.7686 acc_val: 0.7500 time: 0.0143s\n","Epoch: 0170 loss_train: 0.2775 acc_train: 0.9429 loss_val: 0.8097 acc_val: 0.7667 time: 0.0143s\n","Epoch: 0171 loss_train: 0.2208 acc_train: 0.9571 loss_val: 0.7800 acc_val: 0.7867 time: 0.0128s\n","Epoch: 0172 loss_train: 0.2310 acc_train: 0.9571 loss_val: 0.7773 acc_val: 0.7700 time: 0.0135s\n","Epoch: 0173 loss_train: 0.2090 acc_train: 0.9571 loss_val: 0.8352 acc_val: 0.7567 time: 0.0132s\n","Epoch: 0174 loss_train: 0.2220 acc_train: 0.9500 loss_val: 0.7992 acc_val: 0.7833 time: 0.0128s\n","Epoch: 0175 loss_train: 0.2235 acc_train: 0.9571 loss_val: 0.8202 acc_val: 0.7400 time: 0.0130s\n","Epoch: 0176 loss_train: 0.2470 acc_train: 0.9500 loss_val: 0.7435 acc_val: 0.7767 time: 0.0135s\n","Epoch: 0177 loss_train: 0.2244 acc_train: 0.9500 loss_val: 0.8295 acc_val: 0.7667 time: 0.0128s\n","Epoch: 0178 loss_train: 0.2512 acc_train: 0.9357 loss_val: 0.8289 acc_val: 0.7433 time: 0.0129s\n","Epoch: 0179 loss_train: 0.2695 acc_train: 0.9500 loss_val: 0.7930 acc_val: 0.7867 time: 0.0129s\n","Epoch: 0180 loss_train: 0.2349 acc_train: 0.9500 loss_val: 0.8758 acc_val: 0.7367 time: 0.0129s\n","Epoch: 0181 loss_train: 0.2438 acc_train: 0.9643 loss_val: 0.8174 acc_val: 0.7433 time: 0.0120s\n","Epoch: 0182 loss_train: 0.2749 acc_train: 0.9286 loss_val: 0.8380 acc_val: 0.7400 time: 0.0134s\n","Epoch: 0183 loss_train: 0.2629 acc_train: 0.9357 loss_val: 0.8027 acc_val: 0.7867 time: 0.0152s\n","Epoch: 0184 loss_train: 0.2185 acc_train: 0.9643 loss_val: 0.7773 acc_val: 0.7600 time: 0.0131s\n","Epoch: 0185 loss_train: 0.2130 acc_train: 0.9500 loss_val: 0.8590 acc_val: 0.7633 time: 0.0127s\n","Epoch: 0186 loss_train: 0.2047 acc_train: 0.9714 loss_val: 0.8283 acc_val: 0.7633 time: 0.0119s\n","Epoch: 0187 loss_train: 0.2351 acc_train: 0.9571 loss_val: 0.8260 acc_val: 0.7567 time: 0.0126s\n","Epoch: 0188 loss_train: 0.2109 acc_train: 0.9500 loss_val: 0.8715 acc_val: 0.7367 time: 0.0169s\n","Epoch: 0189 loss_train: 0.2351 acc_train: 0.9571 loss_val: 0.8278 acc_val: 0.7467 time: 0.0128s\n","Epoch: 0190 loss_train: 0.2296 acc_train: 0.9357 loss_val: 0.7877 acc_val: 0.7733 time: 0.0126s\n","Epoch: 0191 loss_train: 0.2245 acc_train: 0.9571 loss_val: 0.7788 acc_val: 0.7667 time: 0.0131s\n","Epoch: 0192 loss_train: 0.2311 acc_train: 0.9500 loss_val: 0.8755 acc_val: 0.7500 time: 0.0137s\n","Epoch: 0193 loss_train: 0.2479 acc_train: 0.9714 loss_val: 0.8765 acc_val: 0.7233 time: 0.0130s\n","Epoch: 0194 loss_train: 0.2377 acc_train: 0.9286 loss_val: 0.7589 acc_val: 0.7567 time: 0.0122s\n","Epoch: 0195 loss_train: 0.2240 acc_train: 0.9429 loss_val: 0.7937 acc_val: 0.7633 time: 0.0122s\n","Epoch: 0196 loss_train: 0.1955 acc_train: 0.9714 loss_val: 0.8496 acc_val: 0.7500 time: 0.0140s\n","Epoch: 0197 loss_train: 0.1493 acc_train: 0.9714 loss_val: 0.8053 acc_val: 0.7700 time: 0.0124s\n","Epoch: 0198 loss_train: 0.1738 acc_train: 0.9571 loss_val: 0.7172 acc_val: 0.8000 time: 0.0136s\n","Epoch: 0199 loss_train: 0.2234 acc_train: 0.9571 loss_val: 0.8818 acc_val: 0.7367 time: 0.0127s\n","Epoch: 0200 loss_train: 0.1654 acc_train: 0.9571 loss_val: 0.8087 acc_val: 0.7567 time: 0.0127s\n","Optimization Finished!\n","Total time elapsed: 3.5906s\n","Test set results: loss= 0.6270 accuracy= 0.8070\n"]}]}]}